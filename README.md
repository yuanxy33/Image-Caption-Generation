# Image Caption Generation

<h3>Background</h3>
Given a image and five reference caption, an encoder-decoder architecture can be used to generate caption of the given image.

The figure below shows two image (Image 1 & Image 2) with the given reference text

![Image1](https://user-images.githubusercontent.com/67460572/94655314-f5146480-0330-11eb-8626-10f3b7c66bb5.PNG)

![image](https://user-images.githubusercontent.com/67460572/94655884-dcf11500-0331-11eb-88a8-3e995419a4c5.png)


<h3>Data Set</h3>
The Flickr8k image data set can be obtained from https://github.com/jbrownlee/Datasets/releases/tag/Flickr8k

<h3>Methods</h3>
The encoder learns the compressed encoding features using Pretrained CNN architecture RESTNET and subsequently feed into a RNN / LSTM network.
The RNN/LSTM network uses sequence of words as input and output.

![image](https://user-images.githubusercontent.com/67460572/94655544-55a3a180-0331-11eb-8f86-c2c5d13ca8e2.png)

Encoder extracts image features using the modified resnet-152 pretrained model and feed into the decoder. The image needs to be resized and normalized to 224x224 as per the input of the resnet-152. 

Next, the caption in the data set was cleaned by removing punctuation and lower-case the text. This is to ensure the word embedding only embed the text and ignore the variation of capital and lowercase. A vocab size of 3421 was built on words appears more than 3 times in the caption. The dataset was split into train and testing data were split into ration of 95:5. The word is vectorized into one-hot encoded words.

The decoder then uses the one hot representation of the caption and the features vector as the input to train the model. The loss of RNN and LSTM decreased on every epoch. This is because the feed-backwards network in both networks are learning from the gradient descent backpropagation.

The quality of the generated caption is evaluated through BLEU score (reference: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/#:~:text=Crash%2DCourse%20Now-,Bilingual%20Evaluation%20Understudy%20Score,in%20a%20score%20of%200.0.)


<h2>Result</h2>

![RNNLSTM loss](https://user-images.githubusercontent.com/67460572/94655420-22f9a900-0331-11eb-802f-914d66d21e03.PNG)

The loss in RNN & LSTM drop with epochs, indicating the network is learning. The loss of RNN and LSTM has roughly the same pattern, both drop drastically from 8 to 3 from after first epoch finally reached around 2.3 after 5 epochs. As expected, the BLEU score is terrible before training the model. The output is determined by random initialization of the weight in the neural network.

Evaluating the performance while training, RNN BLEU scores are in the range of 0.41-0.57 while LSTM are in the range of 0.43-0.67. For the second image, RNN scores ranging from 0.41-0.56 while LSTM ranging from 0.39 – 0.73. This suggest that LSTM has a higher BLEU scores than RNN.

![caption generated per epoch](https://user-images.githubusercontent.com/67460572/94650849-0ad25b80-032a-11eb-8811-4de6c0afbb8f.PNG)

The BLEU score generally increased with more training. 

<h3>LSTM vs RNN</h3>

![image](https://user-images.githubusercontent.com/67460572/94655975-04e07880-0332-11eb-8397-9b4174fa9dd6.png)

We will now compare the performance of LSTM and RNN using Image 1 & Image 2. Although in the image 1, both RNN and LSTM has similar BLEU score, LSTM can decode the children are sliding on the slide. In Image 2, both model both wrongly predict the presence of dog in the image, Although RNN has a higher BLEU score, LSTM that describing the person running on the field seems makes more sense.

In human evaluation, LSTM appeared to be performing better in captioning the image. While BLEU is an automated evaluation metrics, the scores is solely dependent on the ground truth text and the generated caption. This suggesting BLEU score should not be the sole evaluation metrics to the performance of the model

![image](https://user-images.githubusercontent.com/67460572/94656149-440ec980-0332-11eb-837e-7877df7bd66f.png)

Apart from BLEU score, the caption generated by LSTM appeared to be better in quality particularly in describing the action. For example, it precisely describes the person is jumping rather than running in image 3 and the person is standing rather than climbing in image 4. LSTM also precisely describe the girl in image 8 is holding something rather than smiling.

LSTM are known to be able to handle long-term dependency learning better than RNN. Due to the gradient vanishing in RNN, RNN are not able to learn and capture the relationship in long sentence.

The reference caption usually involves describing the target’s action in a location. Therefore, the accuracy of describing target-action in a long reference can provide some insight of the long-term dependency. For example, in image 2, 3, 7 and 8, LSTM can precisely describe the action despite the distance of the target-action is longer in the sentence.
